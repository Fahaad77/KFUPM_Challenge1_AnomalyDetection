{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "sD1gc0hxXSei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04axZ-8fFEh3",
        "outputId": "6b534af2-e839-47a0-d024-6414992e2e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set environment"
      ],
      "metadata": {
        "id": "tUi9ohjWXi9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aLkibqglHo8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"AnomalyDetection\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the file path from Google Drive"
      ],
      "metadata": {
        "id": "LEvORihKX2L8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TVWDRk5FEyU"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/combined_output_cleaned_part_2.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "kxVOiDvkYKIO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_reWJtCh65A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import PCA as PCAml\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read CSV using PySpark"
      ],
      "metadata": {
        "id": "mHsa1chqYOoQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfNclpd6Fcd5",
        "outputId": "8af6f039-696d-476b-d7fa-17a95efc7859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+--------+---------+---------+-----+----+-----+---+----+------+------+\n",
            "|P-PDG|     P-TPT|   T-TPT|P-MON-CKP|T-JUS-CKP|class|year|month|day|hour|minute|second|\n",
            "+-----+----------+--------+---------+---------+-----+----+-----+---+----+------+------+\n",
            "|    0|1.009211E7|119.0944|1609800.0| 84.59782|    0|2017|    2|  1|   2|     2|     7|\n",
            "|    0|  1.0092E7|119.0944|1618206.0| 84.58997|    0|2017|    2|  1|   2|     2|     8|\n",
            "|    0|1.009189E7|119.0944|1626612.0| 84.58213|    0|2017|    2|  1|   2|     2|     9|\n",
            "|    0|1.009178E7|119.0944|1635018.0| 84.57429|    0|2017|    2|  1|   2|     2|    10|\n",
            "|    0|1.009167E7|119.0944|1643424.0| 84.56644|    0|2017|    2|  1|   2|     2|    11|\n",
            "+-----+----------+--------+---------+---------+-----+----+-----+---+----+------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlSjuXbbljic"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine features into a single vector"
      ],
      "metadata": {
        "id": "rIob0NXPYU28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fb3jtX9ljpS"
      },
      "outputs": [],
      "source": [
        "feature_cols = [col for col in df.columns if col != 'class']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize features"
      ],
      "metadata": {
        "id": "WOqzMG7lYr3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVqw_Bxdlj4k"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=True, withStd=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKRQzK9YmATq"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=[assembler, scaler])\n",
        "processed_df = pipeline.fit(df).transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into train and test"
      ],
      "metadata": {
        "id": "bOwyFaWWYxAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1wqYyRgFcmi"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = processed_df.randomSplit([0.67, 0.33], seed=2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply PCA to reduce dimensions"
      ],
      "metadata": {
        "id": "wv7sUpYVY28r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6j52303ERQE"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "\n",
        "pca = PCA(k=9, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
        "pca_model = pca.fit(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform the training data"
      ],
      "metadata": {
        "id": "4kZQda4kZUGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGizDCkADGtN",
        "outputId": "659f55b6-a640-4024-f353-1896973b3d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|         pcaFeatures|\n",
            "+--------------------+\n",
            "|[0.85078772242648...|\n",
            "|[0.85061678745645...|\n",
            "|[0.85044619834633...|\n",
            "|[0.85029930335095...|\n",
            "|[0.85015253700283...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_pca_df = pca_model.transform(train_df)\n",
        "train_pca_df.select(\"pcaFeatures\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6EUFEsBpGcQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate reconstruction Error\n"
      ],
      "metadata": {
        "id": "p7MYG3DyZdNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-sFLw3gDGxR"
      },
      "outputs": [],
      "source": [
        "def reconstruct_pca(pca_features, pca_components):\n",
        "    pca_features = np.array(pca_features)  # Convert to NumPy array\n",
        "    pca_components_reduced = pca_components[:pca_features.shape[0], :]  # Take only the first k rows\n",
        "    reconstructed = np.dot(pca_features, pca_components_reduced.T)  # Reconstruct the original features\n",
        "    return float(np.sum((reconstructed - pca_features) ** 2))  # Return reconstruction error as float"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert PCA components to NumPy array"
      ],
      "metadata": {
        "id": "F552_L47aor0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNLCZzRmDGzr"
      },
      "outputs": [],
      "source": [
        "pca_components = np.array(pca_model.pc.toArray())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define UDF for reconstruction error"
      ],
      "metadata": {
        "id": "CvDicuQBatfm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQChEHN4DG3x"
      },
      "outputs": [],
      "source": [
        "reconstruction_udf = udf(lambda pca_feat: reconstruct_pca(pca_feat, pca_components), FloatType())\n",
        "train_pca_df = train_pca_df.withColumn(\"reconstructionError\", reconstruction_udf(\"pcaFeatures\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply reconstruction error as UDF"
      ],
      "metadata": {
        "id": "ujeDTGPNaxX6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQdv394TsDMW"
      },
      "outputs": [],
      "source": [
        "reconstruction_udf = udf(lambda pca_feat: reconstruct_pca(pca_feat, pca_components), FloatType())\n",
        "train_pca_df = train_pca_df.withColumn(\"reconstructionError\", reconstruction_udf(\"pcaFeatures\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize anomaly scores"
      ],
      "metadata": {
        "id": "4X0ecrOKa1dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjkIaQLUDG5I",
        "outputId": "a5103a48-a226-449c-8d1d-5e40f94bf6ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+--------+---------+---------+-----+----+-----+---+----+------+------+--------------------+--------------------+--------------------+-------------------+------------+\n",
            "|     P-PDG|     P-TPT|   T-TPT|P-MON-CKP|T-JUS-CKP|class|year|month|day|hour|minute|second|            features|      scaledFeatures|         pcaFeatures|reconstructionError|anomalyScore|\n",
            "+----------+----------+--------+---------+---------+-----+----+-----+---+----+------+------+--------------------+--------------------+--------------------+-------------------+------------+\n",
            "|-125436200|1.421613E7|116.9995|6055496.0|  69.7748|    0|2017|    6| 27|   0|    58|    23|[-1.254362E8,1.42...|[-0.8629609262824...|[0.85078772242648...|          19.000492| 0.002418188|\n",
            "|-124884800|1.421597E7|116.9997|6055371.0| 69.77486|    0|2017|    6| 27|   0|    58|    22|[-1.248848E8,1.42...|[-0.8593846718647...|[0.85061678745645...|          18.947947|0.0024114272|\n",
            "|-124333400| 1.42158E7|116.9999|6055247.0| 69.77493|    0|2017|    6| 27|   0|    58|    21|[-1.243334E8,1.42...|[-0.8558084174470...|[0.85044619834633...|          18.902088|0.0024055268|\n",
            "|-123782100|1.421564E7|117.0001|6055424.0| 69.77499|    0|2017|    6| 27|   0|    58|    20|[-1.237821E8,1.42...|[-0.8522328116064...|[0.85029930335095...|          18.863071| 0.002400507|\n",
            "|-123230700|1.421547E7|117.0003|6055603.0| 69.77505|    0|2017|    6| 27|   0|    58|    19|[-1.232307E8,1.42...|[-0.8486565571887...|[0.85015253700283...|          18.830732| 0.002396346|\n",
            "+----------+----------+--------+---------+---------+-----+----+-----+---+----+------+------+--------------------+--------------------+--------------------+-------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "min_error = train_pca_df.agg({\"reconstructionError\": \"min\"}).collect()[0][0]\n",
        "max_error = train_pca_df.agg({\"reconstructionError\": \"max\"}).collect()[0][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuIefBHGEfng",
        "outputId": "3b620805-b75c-4665-adff-53dc816cb6a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+--------+---------+---------+-----+----+-----+---+----+------+------+--------------------+--------------------+--------------------+-------------------+--------------------+\n",
            "|     P-PDG|     P-TPT|   T-TPT|P-MON-CKP|T-JUS-CKP|class|year|month|day|hour|minute|second|            features|      scaledFeatures|         pcaFeatures|reconstructionError|        anomalyScore|\n",
            "+----------+----------+--------+---------+---------+-----+----+-----+---+----+------+------+--------------------+--------------------+--------------------+-------------------+--------------------+\n",
            "|-125436200|1.421613E7|116.9995|6055496.0|  69.7748|    0|2017|    6| 27|   0|    58|    23|[-1.254362E8,1.42...|[-0.8629609262824...|[0.85078772242648...|          19.000492|0.002418187904463...|\n",
            "|-124884800|1.421597E7|116.9997|6055371.0| 69.77486|    0|2017|    6| 27|   0|    58|    22|[-1.248848E8,1.42...|[-0.8593846718647...|[0.85061678745645...|          18.947947|0.002411427266089...|\n",
            "|-124333400| 1.42158E7|116.9999|6055247.0| 69.77493|    0|2017|    6| 27|   0|    58|    21|[-1.243334E8,1.42...|[-0.8558084174470...|[0.85044619834633...|          18.902088|0.002405526924713...|\n",
            "|-123782100|1.421564E7|117.0001|6055424.0| 69.77499|    0|2017|    6| 27|   0|    58|    20|[-1.237821E8,1.42...|[-0.8522328116064...|[0.85029930335095...|          18.863071|0.002400507066914...|\n",
            "|-123230700|1.421547E7|117.0003|6055603.0| 69.77505|    0|2017|    6| 27|   0|    58|    19|[-1.232307E8,1.42...|[-0.8486565571887...|[0.85015253700283...|          18.830732|0.002396345984961...|\n",
            "+----------+----------+--------+---------+---------+-----+----+-----+---+----+------+------+--------------------+--------------------+--------------------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "normalize_udf = udf(lambda x: (x - min_error) / (max_error - min_error), DoubleType())\n",
        "train_pca_df = train_pca_df.withColumn(\"anomalyScore\", normalize_udf(\"reconstructionError\"))\n",
        "train_pca_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate using AUC"
      ],
      "metadata": {
        "id": "3GuLPt3ubEEe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH9QuHKQDG9x"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "# Prepare the labels and anomaly scores\n",
        "evaluation_df = train_pca_df.selectExpr(\"class as label\", \"anomalyScore as prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yk4py4FFpaVV",
        "outputId": "ff74f369-32ee-4b4b-8f6b-84213d65757b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.5711606138328211\n"
          ]
        }
      ],
      "source": [
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(evaluation_df)\n",
        "print(f\"AUC: {auc}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}